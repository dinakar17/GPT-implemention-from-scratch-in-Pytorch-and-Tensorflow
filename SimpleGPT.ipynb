{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4gjmLSf0smm5"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Hyperparameters\n",
        "'''\n",
        "\n",
        "batch_size = 128 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum content length for predictions?\n",
        "\n",
        "n_embd=64 # embedding dimension (vector size of each token)\n",
        "dropout=0.2 # dropout rate\n",
        "\n",
        "n_layer=2 # number of decoder blocks\n",
        "n_head=4 # Number of attention heads\n",
        "# head_size = n_embd / n_head = 64/4 = 16\n",
        "\n",
        "# no epochs are there for the model (since we're sampling random batches)\n",
        "max_iters = 5000 # number of iterations \n",
        "eval_interval = 300 # After how many iterations train loss and val loss are calculated\n",
        "eval_iters = 200 # how many random batches are sampled to calculate the train and val loss\n",
        "\n",
        "learning_rate = 2e-3 # with baby networks can afford to go a bit higher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRfi4Jajg5Cn",
        "outputId": "4ffc0926-0c36-45c3-9f4c-6e1ea9fe2476"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of dataset in characters:  6622027\n"
          ]
        }
      ],
      "source": [
        "import urllib.request\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/dinakar17/GPT-implemention-from-scratch-in-Pytorch-and-Tensorflow/main/data/HPBooks%20Dataset/HPBooks(1-7).txt\"\n",
        "\n",
        "# Using with context manager to read the file contents\n",
        "with urllib.request.urlopen(url) as response:\n",
        "    text = response.read().decode('utf-8')\n",
        "\n",
        "print(\"Length of dataset in characters: \", len(text)) # ~ 6.6 million tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RG8jX3p6hDFn",
        "outputId": "ed888580-0ab9-40a1-bf61-105a9ad48da7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "THE BOY WHO LIVED\n",
            "\n",
            "Mr. and Mrs. Dursley, of number four, Privet Drive,\n",
            "were proud to say that they were perfectly normal,\n",
            "thank you very much. They were the last people you’d\n",
            "expect to be involved in anything strange or\n",
            "mysterious, because they just didn’t hold with such\n",
            "nonsense.\n",
            "\n",
            "Mr. Dursley was the director of a firm called\n",
            "Grunnings, which made drills. He was a big, beefy\n",
            "man with hardly any neck, although he did have a\n",
            "very large mustache. Mrs. Dursley was thin and\n",
            "blonde and had nearly twice the usual amount of\n",
            "neck, which came in very useful as she spent so\n",
            "much of her time craning over garden fences, spying\n",
            "on the neighbors. The Dursley s had a small son\n",
            "called Dudley and in their opinion there was no finer\n",
            "boy anywhere.\n",
            "\n",
            "The Dursleys had everything they wanted, but they\n",
            "also had a secret, and their greatest fear was that\n",
            "somebody would discover it. They didn’t think they\n",
            "could bear it if anyone found out about the Potters.\n",
            "Mrs. Potter was Mrs. Dursley’s sister, but they\n"
          ]
        }
      ],
      "source": [
        "# First 1000 characters\n",
        "print(text[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9AMP1qMhEz6",
        "outputId": "0adeeafd-cd5d-42c5-846f-8f56f63c37d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " !\"%&'()*,-./0123456789:;>?ABCDEFGHIJKLMNOPQRSTUVWXYZ\\]abcdefghijklmnopqrstuvwxyz|~—‘’“”•■□﻿\n",
            "93\n"
          ]
        }
      ],
      "source": [
        "# All unique characters that occurs in the text\n",
        "\n",
        "chars : list = sorted(list(set(text))) \n",
        "# character-level vocabulary\n",
        "vocab_size : int = len(chars) \n",
        "# Unique characters\n",
        "print(''.join(chars))\n",
        "# Total number of unique characters\n",
        "print(vocab_size) # i.e.,for  ~ 6.6 millions of tokens vocabulary size is 93"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ameNYzqhGe9",
        "outputId": "b3c96194-76b4-477e-f079-df3c34f111ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[63, 64, 64, 64, 1, 75, 63, 60, 73, 60]\n",
            "hii there\n"
          ]
        }
      ],
      "source": [
        "# create a mapping from characters to integers\n",
        "\n",
        "from typing import Dict\n",
        "\n",
        "# Assigning a unique integer to each unique character \n",
        "stoi : Dict[str, int] = {ch: i for i, ch in enumerate(chars)}\n",
        "itos : Dict[int, str] = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "from typing import Callable, List\n",
        "\n",
        "encode : Callable[[str], List[int]] = lambda s : [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode : Callable[[List[int]], str] = lambda l : ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "print(encode('hiii there'))\n",
        "print(decode(encode('hii there')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7o50fCzj9Un",
        "outputId": "ef108fea-2e99-429c-f672-7475df14890e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([6622027]) torch.int64\n",
            "tensor([92, 13,  0,  0,  0,  0,  0, 47, 35, 32,  1, 29, 42, 52,  1, 50, 35, 42,\n",
            "         1, 39, 36, 49, 32, 31,  0,  0, 40, 73, 12,  1, 56, 69, 59,  1, 40, 73,\n",
            "        74, 12,  1, 31, 76, 73, 74, 67, 60, 80, 10,  1, 70, 61,  1, 69, 76, 68,\n",
            "        57, 60, 73,  1, 61, 70, 76, 73, 10,  1, 43, 73, 64, 77, 60, 75,  1, 31,\n",
            "        73, 64, 77, 60, 10,  0, 78, 60, 73, 60,  1, 71, 73, 70, 76, 59,  1, 75,\n",
            "        70,  1, 74, 56, 80,  1, 75, 63, 56, 75])\n"
          ]
        }
      ],
      "source": [
        "# Encode the entire text dataset and store it into a torch.Tensor\n",
        "import torch \n",
        "\n",
        "data : torch.Tensor = torch.tensor(encode(text), dtype=torch.long) # convert the list of integers to a torch tensor\n",
        "\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:100]) # the 100 characters we looked at earlier will look to GPT like this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "2TNNsypJn4g4"
      },
      "outputs": [],
      "source": [
        "# Todo: Look for another way to split the data\n",
        "# Split up the data into train and validation sets\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data : torch.Tensor = data[:n]\n",
        "val_data : torch.Tensor = data[n:] # validation data helps us understand to what extent the model is overfitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "NilI7fdnmrEo"
      },
      "outputs": [],
      "source": [
        "from typing import Callable, List, Tuple\n",
        "\n",
        "# get_batch basically samples random batch\n",
        "def get_batch(split : str) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "  # generate a small batch of data of inputs x and targets y \n",
        "  data = train_data if split == 'train' else val_data\n",
        "  # ix is a list of 64 (if batch_size = 64) random integers between 0 and len(data) - block_size (the last possible starting index for a block of size block_size)\n",
        "  ix : torch.Tensor[int, torch.Size[batch_size]] = torch.randint(len(data) - block_size, (batch_size, )) \n",
        "  x : torch.Tensor[int, torch.Size[batch_size, block_size]] = torch.stack([data[i: i+block_size] for i in ix])\n",
        "  y : torch.Tensor[int, torch.Size[batch_size, block_size]] = torch.stack([data[i+1: i+block_size+1] for i in ix])\n",
        "  return x, y\n",
        "\n",
        "# Get one batch of training data\n",
        "xb, yb = get_batch('train')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "N2eeG2mPsG9g"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size: int):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # input of size (batch, time-step, channels)\n",
        "        # output of size (batch, time-step, head size)\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,hs)\n",
        "        q = self.query(x) # (B,T,hs)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,hs)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "TpW4LXeuswIN"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "vNymsaC8zG1d"
      },
      "outputs": [],
      "source": [
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd: int):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Bw1OpjAgzJQY"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd: int, n_head: int):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "# Todo: Residual connections\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x)) # Applying layer normalization earlier\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbXCu1NkzPYe",
        "outputId": "b0818404-363c-4ede-8474-efe0721ce750"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1024, 93])\n",
            "tensor(4.6329, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "A~DeaQL*s\\-!)5|j0-/E(crZ!’;□Uk■x”■vkbYhOXP\"DU5a8L%pAF|S!P—z%LD vvWa?q6yz•|id•J“t%.1Ze6lTMX‘x0FHWH(“*\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "torch.manual_seed(1337) # for same initialization of weight parameters every time\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    # each token directly reads off the logits for the next token from a lookup token\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embd) # embedding dimension = vocab_size (it could be any)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embd) \n",
        "    self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "    self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "    self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    B, T = idx.shape\n",
        "\n",
        "    # idx and targets are both (B, T) tensors of integers\n",
        "    tok_emb = self.token_embedding_table(idx) # (B, T, C)\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T)) # (T, C)\n",
        "    x = tok_emb + pos_emb # (B, T, C)\n",
        "    x = self.blocks(x) # (B, T, C)\n",
        "    x = self.ln_f(x) # (B, T, C)\n",
        "    logits = self.lm_head(x) # (B, T, vocab_size)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits=  logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  \n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    # idx is (B, T) array of indices in the current context\n",
        "    for _ in range(max_new_tokens):\n",
        "        # crop idx to the last block_size tokens\n",
        "        idx_cond = idx[:, -block_size:] # Doubt here\n",
        "        # get the predictions\n",
        "        logits, loss = self(idx_cond)\n",
        "        # focus only on the last time step\n",
        "        logits = logits[:, -1, :] # becomes (B, C)\n",
        "        # apply softmax to get probabilities\n",
        "        probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "        # sample from the distribution\n",
        "        idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "        # append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "    return idx\n",
        "\n",
        "\n",
        "m = GPT(vocab_size)\n",
        "logits, loss = m(xb, yb) # Think of logits as output\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "print(decode(m.generate(torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist())) # output from an untrained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "nOCvSlqizqwP"
      },
      "outputs": [],
      "source": [
        "# create a Pytorch optimizer\n",
        "optimizer = torch.optim.Adam(m.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "gmZdENgR0UGi"
      },
      "outputs": [],
      "source": [
        "eval_iters = 200\n",
        "\n",
        "# here we're \"sampling random 200 batches with replacement\" meaning some batches\n",
        "# may be repeated twice while other batches aren't even considered \n",
        "# this is a common practice for models which deal with gigantic datasets\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "  out = {}\n",
        "  m.eval()\n",
        "  for split in ['train', 'val']:\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "      X, Y = get_batch(split)\n",
        "      logits, loss = m(X, Y)\n",
        "      losses[k] = loss.item()\n",
        "    out[split] = losses.mean()\n",
        "  m.train()\n",
        "  return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "stfAWaag0W2d",
        "outputId": "944afae4-2b6d-4c06-8ae3-24e5472b01c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 0: train loss 4.6295, val loss 4.6310\n",
            "Step 300: train loss 2.2039, val loss 2.1790\n",
            "Step 600: train loss 2.0755, val loss 2.0539\n",
            "Step 900: train loss 2.0040, val loss 1.9970\n",
            "Step 1200: train loss 1.9707, val loss 1.9596\n",
            "Step 1500: train loss 1.9345, val loss 1.9481\n",
            "Step 1800: train loss 1.9119, val loss 1.9187\n",
            "Step 2100: train loss 1.8941, val loss 1.9042\n",
            "Step 2400: train loss 1.8814, val loss 1.8828\n",
            "Step 2700: train loss 1.8764, val loss 1.8872\n",
            "Step 3000: train loss 1.8598, val loss 1.8739\n",
            "Step 3300: train loss 1.8617, val loss 1.8688\n",
            "Step 3600: train loss 1.8482, val loss 1.8603\n",
            "Step 3900: train loss 1.8384, val loss 1.8450\n",
            "Step 4200: train loss 1.8288, val loss 1.8426\n",
            "Step 4500: train loss 1.8251, val loss 1.8447\n",
            "Step 4800: train loss 1.8246, val loss 1.8310\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n  if validation loss is less than training loss than don't panic\\n  since we're sampling 200 batches (eval_iters) for calculating the loss\\n  for both train and val data for computation purpose at the expense of some\\n  deviation from the actual loss across all batches\\n\""
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "note that here we cannot say that we're training the model on all the batches\n",
        "rather we're sampling \"max_iters\" random batches \n",
        "this could lead to selecting same batches more than once or may be left out some batches completely \n",
        "but sampling batches with replacement is advantageous than without replacement since \n",
        "with replacement is that it allows the model to see different versions of the same examples in different batches. \n",
        "This can help prevent the model from overfitting to the training set and can improve generalization performance.\n",
        "'''\n",
        "\n",
        "for iter in range(max_iters):\n",
        "  # every once in a while evaluate the loss on train and val sets\n",
        "  if iter % eval_interval == 0:\n",
        "    losses = estimate_loss()\n",
        "    print(f\"Step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "  # sample a batch of da45ta\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  # evaluate the loss\n",
        "  logits, loss = m(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "'''\n",
        "  if validation loss is less than training loss then don't panic\n",
        "  since we're sampling 200(eval_iters) random batches with replacement for calculating the loss\n",
        "  for both train(80%) and val(just 20%, so almost all are involved while calculating the loss) \n",
        "  data for saving computational resources at the expense of some\n",
        "  deviation that could lead to higher value than the \"actual train loss across all batches\"\n",
        "  scaling the model or increase the number of iterations will mitigate this issue since more \n",
        "  training batches are involved in calculating the loss\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUYr1abJ0l7h",
        "outputId": "59a01b15-46a6-4c76-9e31-a5a9015afeeb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "rave you did poighone of Hermioned the stay then, was backen the room a nright that I was was ried to of then, and thick his\n",
            "abrinistry was bulder of looked of fining of cou, Hearmbagge abre was after, \n",
            "\n",
            "“And that. “Wo’l said just stiusing as like have sood, leap of\n",
            "a suddenordere gocid the,” bane. Duse mort boin,’ quing\n",
            "witop out lookbook of ekes on he could\n",
            "not, it know hie sto this that llike the coul\n",
            "Harry\n",
            "asmibly, Page | 927Hat servingly of her about caup holiso\n",
            "litch maded, burry.”\n",
            "\n",
            "“We- J\n"
          ]
        }
      ],
      "source": [
        "print(decode(m.generate(torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist())) "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "72cd36df6ae966e5b8bdbc170e057f0a4e382a1fc161b1a5aab8ce8b35d1f9c7"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
